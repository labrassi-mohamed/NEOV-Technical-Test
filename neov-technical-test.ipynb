{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10842395,"sourceType":"datasetVersion","datasetId":6733449}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet langchain-google-genai==2.0.9 langchain_community langchain langchain_chroma dotenv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:33:16.906990Z","iopub.execute_input":"2025-02-24T22:33:16.907258Z","iopub.status.idle":"2025-02-24T22:33:47.904856Z","shell.execute_reply.started":"2025-02-24T22:33:16.907238Z","shell.execute_reply":"2025-02-24T22:33:47.903915Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport getpass\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_chroma import Chroma\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:33:47.905995Z","iopub.execute_input":"2025-02-24T22:33:47.906287Z","iopub.status.idle":"2025-02-24T22:33:51.937260Z","shell.execute_reply.started":"2025-02-24T22:33:47.906239Z","shell.execute_reply":"2025-02-24T22:33:51.936315Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if \"GOOGLE_API_KEY\" not in os.environ:\n    os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Function to ingest documents (PDF or text files)\n","metadata":{}},{"cell_type":"code","source":"def ingest_documents(docs_path):\n    file_ext = os.path.splitext(docs_path)[1].lower()\n\n    # Select the appropriate loader based on file extension\n    if file_ext == '.pdf':\n        loader = PyPDFLoader(docs_path)\n    elif file_ext == '.txt':\n        loader = TextLoader(docs_path)\n    else:\n        raise ValueError(f\"Unsupported file format: {file_ext}\")\n\n    # Load the document\n    documents = loader.load()\n    \n    # Split the document into smaller chunks for better processing\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n    documents = text_splitter.split_documents(documents)\n    \n    return documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:33:59.276504Z","iopub.execute_input":"2025-02-24T22:33:59.276769Z","iopub.status.idle":"2025-02-24T22:33:59.281384Z","shell.execute_reply.started":"2025-02-24T22:33:59.276748Z","shell.execute_reply":"2025-02-24T22:33:59.280677Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Function to embed the documents using Google Generative AI and store them in ChromaDB","metadata":{}},{"cell_type":"code","source":"def embed_documents(documents): \n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n    return Chroma.from_documents(documents=documents, embedding=embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:34:01.035575Z","iopub.execute_input":"2025-02-24T22:34:01.035860Z","iopub.status.idle":"2025-02-24T22:34:01.039709Z","shell.execute_reply.started":"2025-02-24T22:34:01.035838Z","shell.execute_reply":"2025-02-24T22:34:01.038841Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Ingest and embed the documents from the specified file","metadata":{}},{"cell_type":"code","source":"vectorstore = embed_documents(ingest_documents(\"your_document_path\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:34:16.887806Z","iopub.execute_input":"2025-02-24T22:34:16.888111Z","iopub.status.idle":"2025-02-24T22:34:18.756487Z","shell.execute_reply.started":"2025-02-24T22:34:16.888084Z","shell.execute_reply":"2025-02-24T22:34:18.755242Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Create a retriever to perform similarity-based document search","metadata":{}},{"cell_type":"code","source":"retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialize the language model (LLM) using Google's Gemini-1.5-Pro","metadata":{}},{"cell_type":"code","source":"llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3, max_tokens=500)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:34:19.647431Z","iopub.execute_input":"2025-02-24T22:34:19.647879Z","iopub.status.idle":"2025-02-24T22:34:19.654606Z","shell.execute_reply.started":"2025-02-24T22:34:19.647856Z","shell.execute_reply":"2025-02-24T22:34:19.653785Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Define the system prompt to guide the LLM's behavior","metadata":{}},{"cell_type":"code","source":"system_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:34:21.469199Z","iopub.execute_input":"2025-02-24T22:34:21.469523Z","iopub.status.idle":"2025-02-24T22:34:21.472878Z","shell.execute_reply.started":"2025-02-24T22:34:21.469496Z","shell.execute_reply":"2025-02-24T22:34:21.472182Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Create a chat prompt template that includes the system prompt and user input","metadata":{}},{"cell_type":"code","source":"prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        (\"human\", \"{input}\"),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:34:23.280351Z","iopub.execute_input":"2025-02-24T22:34:23.280622Z","iopub.status.idle":"2025-02-24T22:34:23.284543Z","shell.execute_reply.started":"2025-02-24T22:34:23.280601Z","shell.execute_reply":"2025-02-24T22:34:23.283861Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Create a question-answering chain using the LLM and prompt","metadata":{}},{"cell_type":"code","source":"question_answer_chain = create_stuff_documents_chain(llm, prompt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:34:24.823625Z","iopub.execute_input":"2025-02-24T22:34:24.823906Z","iopub.status.idle":"2025-02-24T22:34:24.830063Z","shell.execute_reply.started":"2025-02-24T22:34:24.823882Z","shell.execute_reply":"2025-02-24T22:34:24.829360Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Create a full RAG (Retrieval-Augmented Generation) pipeline","metadata":{}},{"cell_type":"code","source":"rag_chain = create_retrieval_chain(retriever, question_answer_chain)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interactive loop for user queries","metadata":{}},{"cell_type":"code","source":"while True:\n    input_message = input(\"Ask a question: \")\n    if input_message.lower() == \"exit\":\n        break\n        \n    response = rag_chain.invoke({\"input\": input_message})\n    print(response[\"answer\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
